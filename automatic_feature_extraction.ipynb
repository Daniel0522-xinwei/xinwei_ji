{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read files\n",
    "data = pd.read_csv(\"C:/Users/Danie/Desktop/biovid/part_A/input_gsr_part_a.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2807</th>\n",
       "      <th>2808</th>\n",
       "      <th>2809</th>\n",
       "      <th>2810</th>\n",
       "      <th>2811</th>\n",
       "      <th>2812</th>\n",
       "      <th>2813</th>\n",
       "      <th>2814</th>\n",
       "      <th>2815</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.05920</td>\n",
       "      <td>13.057800</td>\n",
       "      <td>13.05540</td>\n",
       "      <td>13.054800</td>\n",
       "      <td>13.053000</td>\n",
       "      <td>13.050400</td>\n",
       "      <td>13.05000</td>\n",
       "      <td>13.04760</td>\n",
       "      <td>13.04720</td>\n",
       "      <td>13.04920</td>\n",
       "      <td>...</td>\n",
       "      <td>11.156000</td>\n",
       "      <td>11.159100</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>11.156130</td>\n",
       "      <td>11.158100</td>\n",
       "      <td>11.164420</td>\n",
       "      <td>11.162120</td>\n",
       "      <td>11.160220</td>\n",
       "      <td>11.163100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.15056</td>\n",
       "      <td>10.151000</td>\n",
       "      <td>10.15044</td>\n",
       "      <td>10.151120</td>\n",
       "      <td>10.151440</td>\n",
       "      <td>10.149320</td>\n",
       "      <td>10.15080</td>\n",
       "      <td>10.15244</td>\n",
       "      <td>10.15312</td>\n",
       "      <td>10.15344</td>\n",
       "      <td>...</td>\n",
       "      <td>9.830474</td>\n",
       "      <td>9.831763</td>\n",
       "      <td>9.832000</td>\n",
       "      <td>9.833764</td>\n",
       "      <td>9.830472</td>\n",
       "      <td>9.831764</td>\n",
       "      <td>9.831118</td>\n",
       "      <td>9.830117</td>\n",
       "      <td>9.828234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.06428</td>\n",
       "      <td>10.066280</td>\n",
       "      <td>10.06636</td>\n",
       "      <td>10.065360</td>\n",
       "      <td>10.065640</td>\n",
       "      <td>10.066640</td>\n",
       "      <td>10.06636</td>\n",
       "      <td>10.06472</td>\n",
       "      <td>10.06336</td>\n",
       "      <td>10.06300</td>\n",
       "      <td>...</td>\n",
       "      <td>9.897189</td>\n",
       "      <td>9.896189</td>\n",
       "      <td>9.898434</td>\n",
       "      <td>9.899000</td>\n",
       "      <td>9.899000</td>\n",
       "      <td>9.897376</td>\n",
       "      <td>9.896188</td>\n",
       "      <td>9.897624</td>\n",
       "      <td>9.898000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.38556</td>\n",
       "      <td>11.387280</td>\n",
       "      <td>11.38772</td>\n",
       "      <td>11.386720</td>\n",
       "      <td>11.386280</td>\n",
       "      <td>11.386430</td>\n",
       "      <td>11.38500</td>\n",
       "      <td>11.38472</td>\n",
       "      <td>11.38457</td>\n",
       "      <td>11.38543</td>\n",
       "      <td>...</td>\n",
       "      <td>10.672330</td>\n",
       "      <td>10.674000</td>\n",
       "      <td>10.672840</td>\n",
       "      <td>10.671420</td>\n",
       "      <td>10.669840</td>\n",
       "      <td>10.670750</td>\n",
       "      <td>10.670830</td>\n",
       "      <td>10.671170</td>\n",
       "      <td>10.672580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.72860</td>\n",
       "      <td>10.729800</td>\n",
       "      <td>10.72601</td>\n",
       "      <td>10.725000</td>\n",
       "      <td>10.726600</td>\n",
       "      <td>10.727800</td>\n",
       "      <td>10.72880</td>\n",
       "      <td>10.72660</td>\n",
       "      <td>10.72760</td>\n",
       "      <td>10.72800</td>\n",
       "      <td>...</td>\n",
       "      <td>10.509000</td>\n",
       "      <td>10.509020</td>\n",
       "      <td>10.509940</td>\n",
       "      <td>10.507040</td>\n",
       "      <td>10.509040</td>\n",
       "      <td>10.510890</td>\n",
       "      <td>10.505130</td>\n",
       "      <td>10.511930</td>\n",
       "      <td>10.508000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8696</th>\n",
       "      <td>1.35100</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.35100</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.35100</td>\n",
       "      <td>1.35100</td>\n",
       "      <td>1.35100</td>\n",
       "      <td>1.35100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.355000</td>\n",
       "      <td>1.355142</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>1.34900</td>\n",
       "      <td>1.349000</td>\n",
       "      <td>1.34900</td>\n",
       "      <td>1.348869</td>\n",
       "      <td>1.348131</td>\n",
       "      <td>1.348869</td>\n",
       "      <td>1.34800</td>\n",
       "      <td>1.34800</td>\n",
       "      <td>1.34800</td>\n",
       "      <td>1.34800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>1.317000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>1.263000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>1.25400</td>\n",
       "      <td>1.254214</td>\n",
       "      <td>1.25500</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>1.25500</td>\n",
       "      <td>1.25500</td>\n",
       "      <td>1.25500</td>\n",
       "      <td>1.25500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8700 rows × 2817 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2          3          4          5  \\\n",
       "0     13.05920  13.057800  13.05540  13.054800  13.053000  13.050400   \n",
       "1     10.15056  10.151000  10.15044  10.151120  10.151440  10.149320   \n",
       "2     10.06428  10.066280  10.06636  10.065360  10.065640  10.066640   \n",
       "3     11.38556  11.387280  11.38772  11.386720  11.386280  11.386430   \n",
       "4     10.72860  10.729800  10.72601  10.725000  10.726600  10.727800   \n",
       "...        ...        ...       ...        ...        ...        ...   \n",
       "8695   1.25700   1.257000   1.25700   1.257000   1.257000   1.257000   \n",
       "8696   1.35100   1.351000   1.35100   1.351000   1.351000   1.351000   \n",
       "8697   1.34900   1.349000   1.34900   1.348869   1.348131   1.348869   \n",
       "8698   1.25700   1.257000   1.25700   1.257000   1.257000   1.257000   \n",
       "8699   1.25400   1.254214   1.25500   1.255000   1.255000   1.255000   \n",
       "\n",
       "             6         7         8         9  ...       2807       2808  \\\n",
       "0     13.05000  13.04760  13.04720  13.04920  ...  11.156000  11.159100   \n",
       "1     10.15080  10.15244  10.15312  10.15344  ...   9.830474   9.831763   \n",
       "2     10.06636  10.06472  10.06336  10.06300  ...   9.897189   9.896189   \n",
       "3     11.38500  11.38472  11.38457  11.38543  ...  10.672330  10.674000   \n",
       "4     10.72880  10.72660  10.72760  10.72800  ...  10.509000  10.509020   \n",
       "...        ...       ...       ...       ...  ...        ...        ...   \n",
       "8695   1.25700   1.25700   1.25700   1.25700  ...   1.276000   1.276000   \n",
       "8696   1.35100   1.35100   1.35100   1.35100  ...   1.355000   1.355142   \n",
       "8697   1.34800   1.34800   1.34800   1.34800  ...   1.317000   1.317000   \n",
       "8698   1.25700   1.25700   1.25700   1.25700  ...   1.263000   1.263000   \n",
       "8699   1.25500   1.25500   1.25500   1.25500  ...   1.431000   1.431000   \n",
       "\n",
       "           2809       2810       2811       2812       2813       2814  \\\n",
       "0     11.160000  11.156130  11.158100  11.164420  11.162120  11.160220   \n",
       "1      9.832000   9.833764   9.830472   9.831764   9.831118   9.830117   \n",
       "2      9.898434   9.899000   9.899000   9.897376   9.896188   9.897624   \n",
       "3     10.672840  10.671420  10.669840  10.670750  10.670830  10.671170   \n",
       "4     10.509940  10.507040  10.509040  10.510890  10.505130  10.511930   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "8695   1.276000   1.276000   1.276000   1.276000   1.276000   1.276000   \n",
       "8696   1.356000   1.356000   1.356000   1.356000   1.356000   1.356000   \n",
       "8697   1.317000   1.317000   1.317000   1.317000   1.317000   1.317000   \n",
       "8698   1.263000   1.263000   1.263000   1.263000   1.263000   1.263000   \n",
       "8699   1.431000   1.431000   1.431000   1.431000   1.431000   1.431000   \n",
       "\n",
       "           2815  label  \n",
       "0     11.163100      0  \n",
       "1      9.828234      0  \n",
       "2      9.898000      0  \n",
       "3     10.672580      0  \n",
       "4     10.508000      0  \n",
       "...         ...    ...  \n",
       "8695   1.276000      4  \n",
       "8696   1.356000      4  \n",
       "8697   1.317000      4  \n",
       "8698   1.263000      4  \n",
       "8699   1.431000      4  \n",
       "\n",
       "[8700 rows x 2817 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea = pd.read_csv(\"personlized_standarlized_fea37.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea = df_fea.iloc[:,:-1].to_csv(r\"personlized_standarlized_fea37_wo_lable.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data[\"label\"]\n",
    "data_features = data.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features.values,\n",
    "    data_labels.values,\n",
    "    test_size=0.25,\n",
    "    random_state=1,\n",
    "    stratify=data_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features.to_csv(\"eda_wo_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'personlized_standarlized_fea37_wo_lable.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, train = True):\n",
    "        df = pd.read_csv(path, sep=',',index_col = 0)\n",
    "        df = df.values.reshape(-1, df.shape[1]).astype('float32')\n",
    "        print(df.shape)\n",
    "        # randomly split\n",
    "        self.X_train, self.X_test = train_test_split(df, test_size=0.25, random_state=42)\n",
    "        \n",
    "        if train:\n",
    "            self.x = torch.from_numpy(self.X_train)\n",
    "            self.len=self.x.shape[0]\n",
    "        else:\n",
    "            self.x = torch.from_numpy(self.X_test)\n",
    "            self.len=self.x.shape[0]\n",
    "        del self.X_train\n",
    "        del self.X_test \n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8700, 37)\n",
      "(8700, 37)\n"
     ]
    }
   ],
   "source": [
    "traindata_set=DataBuilder(DATA_PATH, train=True)\n",
    "testdata_set=DataBuilder(DATA_PATH, train=False)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set, batch_size = 128)\n",
    "testloader=DataLoader(dataset=testdata_set, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6525, 37])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_set.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2175, 37])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_set.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6525, 37]), torch.Size([2175, 37]))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.x.shape, testloader.dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in = 37,H = 32,H2 = 16,latent_dim = 10):\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = 37\n",
    "H = 32\n",
    "H2 = 16\n",
    "model = Autoencoder(D_in, H, H2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 200 == 0:        \n",
    "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            if epoch % 200 == 0:        \n",
    "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                    epoch, test_loss / len(testloader.dataset)))\n",
    "            test_losses.append(test_loss / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss: 16.1097\n",
      "====> Epoch: 200 Average test loss: 1.0333\n",
      "====> Epoch: 200 Average test loss: 1.9632\n",
      "====> Epoch: 200 Average test loss: 3.1028\n",
      "====> Epoch: 200 Average test loss: 4.0087\n",
      "====> Epoch: 200 Average test loss: 5.0844\n",
      "====> Epoch: 200 Average test loss: 6.1729\n",
      "====> Epoch: 200 Average test loss: 7.1748\n",
      "====> Epoch: 200 Average test loss: 8.2150\n",
      "====> Epoch: 200 Average test loss: 9.1255\n",
      "====> Epoch: 200 Average test loss: 10.2806\n",
      "====> Epoch: 200 Average test loss: 11.3879\n",
      "====> Epoch: 200 Average test loss: 12.4052\n",
      "====> Epoch: 200 Average test loss: 13.4047\n",
      "====> Epoch: 200 Average test loss: 14.4864\n",
      "====> Epoch: 200 Average test loss: 15.4033\n",
      "====> Epoch: 200 Average test loss: 16.3814\n",
      "====> Epoch: 200 Average test loss: 17.3418\n",
      "====> Epoch: 400 Average training loss: 15.9025\n",
      "====> Epoch: 400 Average test loss: 1.0111\n",
      "====> Epoch: 400 Average test loss: 1.9304\n",
      "====> Epoch: 400 Average test loss: 3.0480\n",
      "====> Epoch: 400 Average test loss: 3.9642\n",
      "====> Epoch: 400 Average test loss: 5.0399\n",
      "====> Epoch: 400 Average test loss: 6.1359\n",
      "====> Epoch: 400 Average test loss: 7.1171\n",
      "====> Epoch: 400 Average test loss: 8.1112\n",
      "====> Epoch: 400 Average test loss: 8.9991\n",
      "====> Epoch: 400 Average test loss: 10.1571\n",
      "====> Epoch: 400 Average test loss: 11.2661\n",
      "====> Epoch: 400 Average test loss: 12.2658\n",
      "====> Epoch: 400 Average test loss: 13.2686\n",
      "====> Epoch: 400 Average test loss: 14.3008\n",
      "====> Epoch: 400 Average test loss: 15.2070\n",
      "====> Epoch: 400 Average test loss: 16.1955\n",
      "====> Epoch: 400 Average test loss: 17.1302\n",
      "====> Epoch: 600 Average training loss: 15.7900\n",
      "====> Epoch: 600 Average test loss: 1.0057\n",
      "====> Epoch: 600 Average test loss: 1.9283\n",
      "====> Epoch: 600 Average test loss: 3.0410\n",
      "====> Epoch: 600 Average test loss: 3.9775\n",
      "====> Epoch: 600 Average test loss: 5.0625\n",
      "====> Epoch: 600 Average test loss: 6.1738\n",
      "====> Epoch: 600 Average test loss: 7.1486\n",
      "====> Epoch: 600 Average test loss: 8.1786\n",
      "====> Epoch: 600 Average test loss: 9.0756\n",
      "====> Epoch: 600 Average test loss: 10.2197\n",
      "====> Epoch: 600 Average test loss: 11.3104\n",
      "====> Epoch: 600 Average test loss: 12.3330\n",
      "====> Epoch: 600 Average test loss: 13.3292\n",
      "====> Epoch: 600 Average test loss: 14.3797\n",
      "====> Epoch: 600 Average test loss: 15.2781\n",
      "====> Epoch: 600 Average test loss: 16.2528\n",
      "====> Epoch: 600 Average test loss: 17.1824\n",
      "====> Epoch: 800 Average training loss: 15.7894\n",
      "====> Epoch: 800 Average test loss: 0.9773\n",
      "====> Epoch: 800 Average test loss: 1.9099\n",
      "====> Epoch: 800 Average test loss: 3.0354\n",
      "====> Epoch: 800 Average test loss: 3.9595\n",
      "====> Epoch: 800 Average test loss: 5.0321\n",
      "====> Epoch: 800 Average test loss: 6.1234\n",
      "====> Epoch: 800 Average test loss: 7.0868\n",
      "====> Epoch: 800 Average test loss: 8.1020\n",
      "====> Epoch: 800 Average test loss: 8.9905\n",
      "====> Epoch: 800 Average test loss: 10.1629\n",
      "====> Epoch: 800 Average test loss: 11.2532\n",
      "====> Epoch: 800 Average test loss: 12.2743\n",
      "====> Epoch: 800 Average test loss: 13.2743\n",
      "====> Epoch: 800 Average test loss: 14.3113\n",
      "====> Epoch: 800 Average test loss: 15.2319\n",
      "====> Epoch: 800 Average test loss: 16.2111\n",
      "====> Epoch: 800 Average test loss: 17.1363\n",
      "====> Epoch: 1000 Average training loss: 15.7051\n",
      "====> Epoch: 1000 Average test loss: 0.9953\n",
      "====> Epoch: 1000 Average test loss: 1.9184\n",
      "====> Epoch: 1000 Average test loss: 3.0258\n",
      "====> Epoch: 1000 Average test loss: 3.9402\n",
      "====> Epoch: 1000 Average test loss: 5.0369\n",
      "====> Epoch: 1000 Average test loss: 6.1304\n",
      "====> Epoch: 1000 Average test loss: 7.1007\n",
      "====> Epoch: 1000 Average test loss: 8.1239\n",
      "====> Epoch: 1000 Average test loss: 9.0242\n",
      "====> Epoch: 1000 Average test loss: 10.1992\n",
      "====> Epoch: 1000 Average test loss: 11.3072\n",
      "====> Epoch: 1000 Average test loss: 12.3480\n",
      "====> Epoch: 1000 Average test loss: 13.3488\n",
      "====> Epoch: 1000 Average test loss: 14.3891\n",
      "====> Epoch: 1000 Average test loss: 15.2953\n",
      "====> Epoch: 1000 Average test loss: 16.2189\n",
      "====> Epoch: 1000 Average test loss: 17.1640\n",
      "====> Epoch: 1200 Average training loss: 15.6899\n",
      "====> Epoch: 1200 Average test loss: 1.0010\n",
      "====> Epoch: 1200 Average test loss: 1.9262\n",
      "====> Epoch: 1200 Average test loss: 3.0116\n",
      "====> Epoch: 1200 Average test loss: 3.9278\n",
      "====> Epoch: 1200 Average test loss: 5.0026\n",
      "====> Epoch: 1200 Average test loss: 6.1111\n",
      "====> Epoch: 1200 Average test loss: 7.0986\n",
      "====> Epoch: 1200 Average test loss: 8.1144\n",
      "====> Epoch: 1200 Average test loss: 9.0028\n",
      "====> Epoch: 1200 Average test loss: 10.1491\n",
      "====> Epoch: 1200 Average test loss: 11.2467\n",
      "====> Epoch: 1200 Average test loss: 12.2645\n",
      "====> Epoch: 1200 Average test loss: 13.2580\n",
      "====> Epoch: 1200 Average test loss: 14.2957\n",
      "====> Epoch: 1200 Average test loss: 15.2271\n",
      "====> Epoch: 1200 Average test loss: 16.1737\n",
      "====> Epoch: 1200 Average test loss: 17.0955\n",
      "====> Epoch: 1400 Average training loss: 15.6867\n",
      "====> Epoch: 1400 Average test loss: 0.9961\n",
      "====> Epoch: 1400 Average test loss: 1.9075\n",
      "====> Epoch: 1400 Average test loss: 3.0268\n",
      "====> Epoch: 1400 Average test loss: 3.9446\n",
      "====> Epoch: 1400 Average test loss: 5.0351\n",
      "====> Epoch: 1400 Average test loss: 6.1190\n",
      "====> Epoch: 1400 Average test loss: 7.0943\n",
      "====> Epoch: 1400 Average test loss: 8.1101\n",
      "====> Epoch: 1400 Average test loss: 9.0233\n",
      "====> Epoch: 1400 Average test loss: 10.2049\n",
      "====> Epoch: 1400 Average test loss: 11.3226\n",
      "====> Epoch: 1400 Average test loss: 12.3451\n",
      "====> Epoch: 1400 Average test loss: 13.3539\n",
      "====> Epoch: 1400 Average test loss: 14.4181\n",
      "====> Epoch: 1400 Average test loss: 15.3371\n",
      "====> Epoch: 1400 Average test loss: 16.3027\n",
      "====> Epoch: 1400 Average test loss: 17.2581\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea = pd.read_csv(\"personlized_standarlized_fea37.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea_tensor = torch.tensor(df_fea.iloc[:,:-1].values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_fea_df = model.encode(df_fea_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(latent_fea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent1_fea = []\n",
    "Latent2_fea = []\n",
    "\n",
    "for i in range(8700):\n",
    "    Latent1_fea.append(latent_fea_df[0][i].detach().numpy())\n",
    "    Latent2_fea.append(latent_fea_df[1][i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200420065684313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_fea.iloc[:,:-1],data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf =  SVR(kernel = 'rbf').fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0292729566225287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Latent2_fea,data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf =  SVR(kernel = 'rbf').fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693707894440727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Latent1_fea,data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf =  SVR(kernel = 'rbf').fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928446</td>\n",
       "      <td>0.666151</td>\n",
       "      <td>-0.131081</td>\n",
       "      <td>1.311142</td>\n",
       "      <td>1.294932</td>\n",
       "      <td>0.125111</td>\n",
       "      <td>-0.913778</td>\n",
       "      <td>1.299365</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>1.578256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597101</td>\n",
       "      <td>1.291756</td>\n",
       "      <td>1.281904</td>\n",
       "      <td>1.277057</td>\n",
       "      <td>-2.653598</td>\n",
       "      <td>1.796716</td>\n",
       "      <td>1.798512</td>\n",
       "      <td>-0.208165</td>\n",
       "      <td>-0.239189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.611319</td>\n",
       "      <td>1.955309</td>\n",
       "      <td>-1.094680</td>\n",
       "      <td>-0.588039</td>\n",
       "      <td>0.876183</td>\n",
       "      <td>0.122093</td>\n",
       "      <td>-0.901545</td>\n",
       "      <td>0.301063</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.160771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361909</td>\n",
       "      <td>-0.466768</td>\n",
       "      <td>-0.466479</td>\n",
       "      <td>-0.466306</td>\n",
       "      <td>-0.807866</td>\n",
       "      <td>-0.288885</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>-1.409885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.755482</td>\n",
       "      <td>1.893784</td>\n",
       "      <td>1.159027</td>\n",
       "      <td>0.681295</td>\n",
       "      <td>1.169746</td>\n",
       "      <td>0.114634</td>\n",
       "      <td>-0.643541</td>\n",
       "      <td>0.987774</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.420515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408134</td>\n",
       "      <td>-0.621565</td>\n",
       "      <td>-0.619554</td>\n",
       "      <td>-0.618542</td>\n",
       "      <td>-0.624854</td>\n",
       "      <td>-0.495681</td>\n",
       "      <td>-0.493847</td>\n",
       "      <td>-0.651335</td>\n",
       "      <td>-0.826351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.244110</td>\n",
       "      <td>-1.325378</td>\n",
       "      <td>0.967899</td>\n",
       "      <td>0.858038</td>\n",
       "      <td>1.649315</td>\n",
       "      <td>0.124753</td>\n",
       "      <td>-0.913778</td>\n",
       "      <td>0.630805</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>0.643974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184722</td>\n",
       "      <td>-0.034207</td>\n",
       "      <td>-0.037769</td>\n",
       "      <td>-0.039488</td>\n",
       "      <td>-1.266027</td>\n",
       "      <td>0.228818</td>\n",
       "      <td>0.231328</td>\n",
       "      <td>-0.578957</td>\n",
       "      <td>-0.379920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469802</td>\n",
       "      <td>0.251688</td>\n",
       "      <td>0.792700</td>\n",
       "      <td>0.917488</td>\n",
       "      <td>1.200234</td>\n",
       "      <td>0.117917</td>\n",
       "      <td>-0.141991</td>\n",
       "      <td>0.721560</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.548163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407791</td>\n",
       "      <td>-0.545389</td>\n",
       "      <td>-0.544635</td>\n",
       "      <td>-0.544236</td>\n",
       "      <td>-0.688561</td>\n",
       "      <td>-0.423695</td>\n",
       "      <td>-0.425228</td>\n",
       "      <td>-0.509898</td>\n",
       "      <td>-0.524934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>-0.398736</td>\n",
       "      <td>-0.544932</td>\n",
       "      <td>-1.853470</td>\n",
       "      <td>-1.356849</td>\n",
       "      <td>-0.775205</td>\n",
       "      <td>0.248865</td>\n",
       "      <td>0.656794</td>\n",
       "      <td>-1.643887</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.487086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348145</td>\n",
       "      <td>-0.239510</td>\n",
       "      <td>-0.243129</td>\n",
       "      <td>-0.244856</td>\n",
       "      <td>-0.072112</td>\n",
       "      <td>-0.576452</td>\n",
       "      <td>-0.576375</td>\n",
       "      <td>1.093219</td>\n",
       "      <td>1.138120</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8696</th>\n",
       "      <td>0.946269</td>\n",
       "      <td>1.465741</td>\n",
       "      <td>-1.620734</td>\n",
       "      <td>-1.847518</td>\n",
       "      <td>-0.196470</td>\n",
       "      <td>0.654241</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.820403</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.718160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374992</td>\n",
       "      <td>-0.700658</td>\n",
       "      <td>-0.693167</td>\n",
       "      <td>-0.689540</td>\n",
       "      <td>-0.212206</td>\n",
       "      <td>-0.751143</td>\n",
       "      <td>-0.751121</td>\n",
       "      <td>-0.066412</td>\n",
       "      <td>-1.128885</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>-0.282872</td>\n",
       "      <td>-0.399890</td>\n",
       "      <td>0.509691</td>\n",
       "      <td>0.479731</td>\n",
       "      <td>-0.397822</td>\n",
       "      <td>-0.267623</td>\n",
       "      <td>0.230450</td>\n",
       "      <td>-0.230343</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.419859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349018</td>\n",
       "      <td>-0.414922</td>\n",
       "      <td>-0.414078</td>\n",
       "      <td>-0.413655</td>\n",
       "      <td>-0.582454</td>\n",
       "      <td>-0.414238</td>\n",
       "      <td>-0.414111</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.749645</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>1.205448</td>\n",
       "      <td>1.246944</td>\n",
       "      <td>-1.280582</td>\n",
       "      <td>-1.255426</td>\n",
       "      <td>-1.026854</td>\n",
       "      <td>0.424208</td>\n",
       "      <td>1.329968</td>\n",
       "      <td>-0.476772</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.653296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376450</td>\n",
       "      <td>-0.709222</td>\n",
       "      <td>-0.701548</td>\n",
       "      <td>-0.697834</td>\n",
       "      <td>-0.202199</td>\n",
       "      <td>-0.738665</td>\n",
       "      <td>-0.738639</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>-0.329859</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>1.066321</td>\n",
       "      <td>0.702654</td>\n",
       "      <td>-0.788257</td>\n",
       "      <td>1.705032</td>\n",
       "      <td>-0.225530</td>\n",
       "      <td>0.313667</td>\n",
       "      <td>-1.340290</td>\n",
       "      <td>2.305823</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>0.800570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310816</td>\n",
       "      <td>0.716061</td>\n",
       "      <td>0.705475</td>\n",
       "      <td>0.700517</td>\n",
       "      <td>1.508950</td>\n",
       "      <td>1.395067</td>\n",
       "      <td>1.394424</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>-1.846524</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8700 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0.928446  0.666151 -0.131081  1.311142  1.294932  0.125111 -0.913778   \n",
       "1     0.611319  1.955309 -1.094680 -0.588039  0.876183  0.122093 -0.901545   \n",
       "2     1.755482  1.893784  1.159027  0.681295  1.169746  0.114634 -0.643541   \n",
       "3    -1.244110 -1.325378  0.967899  0.858038  1.649315  0.124753 -0.913778   \n",
       "4     0.469802  0.251688  0.792700  0.917488  1.200234  0.117917 -0.141991   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8695 -0.398736 -0.544932 -1.853470 -1.356849 -0.775205  0.248865  0.656794   \n",
       "8696  0.946269  1.465741 -1.620734 -1.847518 -0.196470  0.654241  0.477281   \n",
       "8697 -0.282872 -0.399890  0.509691  0.479731 -0.397822 -0.267623  0.230450   \n",
       "8698  1.205448  1.246944 -1.280582 -1.255426 -1.026854  0.424208  1.329968   \n",
       "8699  1.066321  0.702654 -0.788257  1.705032 -0.225530  0.313667 -1.340290   \n",
       "\n",
       "            V8       V10       V11  ...         7         8         9  \\\n",
       "0     1.299365 -0.147829  1.578256  ...  0.597101  1.291756  1.281904   \n",
       "1     0.301063 -0.147829 -0.160771  ... -0.361909 -0.466768 -0.466479   \n",
       "2     0.987774 -0.147829 -0.420515  ... -0.408134 -0.621565 -0.619554   \n",
       "3     0.630805 -0.147829  0.643974  ... -0.184722 -0.034207 -0.037769   \n",
       "4     0.721560 -0.147829 -0.548163  ... -0.407791 -0.545389 -0.544635   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8695 -1.643887 -0.100504 -0.487086  ... -0.348145 -0.239510 -0.243129   \n",
       "8696  0.820403 -0.100504 -0.718160  ... -0.374992 -0.700658 -0.693167   \n",
       "8697 -0.230343 -0.100504 -0.419859  ... -0.349018 -0.414922 -0.414078   \n",
       "8698 -0.476772 -0.100504 -0.653296  ... -0.376450 -0.709222 -0.701548   \n",
       "8699  2.305823 -0.100504  0.800570  ...  0.310816  0.716061  0.705475   \n",
       "\n",
       "            10        11        12        13        14        15  label  \n",
       "0     1.277057 -2.653598  1.796716  1.798512 -0.208165 -0.239189      0  \n",
       "1    -0.466306 -0.807866 -0.288885 -0.289532  0.030758 -1.409885      0  \n",
       "2    -0.618542 -0.624854 -0.495681 -0.493847 -0.651335 -0.826351      0  \n",
       "3    -0.039488 -1.266027  0.228818  0.231328 -0.578957 -0.379920      0  \n",
       "4    -0.544236 -0.688561 -0.423695 -0.425228 -0.509898 -0.524934      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "8695 -0.244856 -0.072112 -0.576452 -0.576375  1.093219  1.138120      4  \n",
       "8696 -0.689540 -0.212206 -0.751143 -0.751121 -0.066412 -1.128885      4  \n",
       "8697 -0.413655 -0.582454 -0.414238 -0.414111  0.013588  0.749645      4  \n",
       "8698 -0.697834 -0.202199 -0.738665 -0.738639  0.002608 -0.329859      4  \n",
       "8699  0.700517  1.508950  1.395067  1.394424  0.410072 -1.846524      4  \n",
       "\n",
       "[8700 rows x 38 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of SVR: 0.9200420065684313\n",
      "Mean Absolute Error of LogisticAT: 1.0827586206896551\n",
      "Mean Absolute Error of RandomForestRegressor: 0.9143117486007567\n",
      "Mean Absolute Error of lassoRegressor: 1.008192796338677\n",
      "Mean Absolute Error of ElasticNet: 1.1512338498814052\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_fea.iloc[:,:-1],data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mord\n",
    "mae_SVR = []\n",
    "mae_Logitstic_AT = []\n",
    "mae_RandomForestRegressor = []\n",
    "mae_lasso = []\n",
    "mae_ElasticNet = []\n",
    "\n",
    "#SVR\n",
    "SVR_regressor = SVR(kernel = 'rbf').fit(X_train, y_train)\n",
    "mae_SVR.append(mean_absolute_error(SVR_regressor.predict(X_test), y_test))\n",
    "\n",
    "#value must be [0,1]\n",
    "mord_LogisticAT =  mord.LogisticIT(alpha=1).fit(X_train,y_train)\n",
    "mae_Logitstic_AT.append(mean_absolute_error(mord_LogisticAT.predict(X_test), y_test))\n",
    "\n",
    "#RandomForestRegressor\n",
    "RandomForestRegressor = RandomForestRegressor(n_estimators =100, max_depth=10, random_state=0).fit(X_train, y_train)\n",
    "mae_RandomForestRegressor.append(mean_absolute_error(RandomForestRegressor.predict(X_test), y_test))\n",
    "\n",
    "#linear lasso\n",
    "lasso_regressor = linear_model.Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "mae_lasso.append(mean_absolute_error(lasso_regressor.predict(X_test), y_test))\n",
    "\n",
    "#linear-elasticNet\n",
    "ElasticnNet_regressor = linear_model.ElasticNet(random_state = 0).fit(X_train, y_train)\n",
    "mae_ElasticNet.append(mean_absolute_error(ElasticnNet_regressor.predict(X_test), y_test))\n",
    "\n",
    "print('Mean Absolute Error of SVR: %s' % np.mean(mae_SVR))\n",
    "print('Mean Absolute Error of LogisticAT: %s' % np.mean(mae_Logitstic_AT))\n",
    "print('Mean Absolute Error of RandomForestRegressor: %s' % np.mean(mae_RandomForestRegressor))\n",
    "print('Mean Absolute Error of lassoRegressor: %s' % np.mean(mae_lasso))\n",
    "print('Mean Absolute Error of ElasticNet: %s' % np.mean(mae_ElasticNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df_fea.iloc[:,:-1], pd.DataFrame(Latent1_fea)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928446</td>\n",
       "      <td>0.666151</td>\n",
       "      <td>-0.131081</td>\n",
       "      <td>1.311142</td>\n",
       "      <td>1.294932</td>\n",
       "      <td>0.125111</td>\n",
       "      <td>-0.913778</td>\n",
       "      <td>1.299365</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>1.578256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002328</td>\n",
       "      <td>-0.007123</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>-0.010898</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>0.477527</td>\n",
       "      <td>-0.006696</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>2.181365</td>\n",
       "      <td>-1.147782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.611319</td>\n",
       "      <td>1.955309</td>\n",
       "      <td>-1.094680</td>\n",
       "      <td>-0.588039</td>\n",
       "      <td>0.876183</td>\n",
       "      <td>0.122093</td>\n",
       "      <td>-0.901545</td>\n",
       "      <td>0.301063</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.160771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>-0.012644</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>-0.566121</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>-0.814793</td>\n",
       "      <td>-0.868933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.755482</td>\n",
       "      <td>1.893784</td>\n",
       "      <td>1.159027</td>\n",
       "      <td>0.681295</td>\n",
       "      <td>1.169746</td>\n",
       "      <td>0.114634</td>\n",
       "      <td>-0.643541</td>\n",
       "      <td>0.987774</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.420515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006163</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>0.006907</td>\n",
       "      <td>-0.011537</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>-1.375672</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.278375</td>\n",
       "      <td>-1.255553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.244110</td>\n",
       "      <td>-1.325378</td>\n",
       "      <td>0.967899</td>\n",
       "      <td>0.858038</td>\n",
       "      <td>1.649315</td>\n",
       "      <td>0.124753</td>\n",
       "      <td>-0.913778</td>\n",
       "      <td>0.630805</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>0.643974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003119</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>-0.312640</td>\n",
       "      <td>-0.016857</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>1.877005</td>\n",
       "      <td>0.491665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469802</td>\n",
       "      <td>0.251688</td>\n",
       "      <td>0.792700</td>\n",
       "      <td>0.917488</td>\n",
       "      <td>1.200234</td>\n",
       "      <td>0.117917</td>\n",
       "      <td>-0.141991</td>\n",
       "      <td>0.721560</td>\n",
       "      <td>-0.147829</td>\n",
       "      <td>-0.548163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>-0.003955</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>-0.929842</td>\n",
       "      <td>-0.008932</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>0.458964</td>\n",
       "      <td>-0.206948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>-0.398736</td>\n",
       "      <td>-0.544932</td>\n",
       "      <td>-1.853470</td>\n",
       "      <td>-1.356849</td>\n",
       "      <td>-0.775205</td>\n",
       "      <td>0.248865</td>\n",
       "      <td>0.656794</td>\n",
       "      <td>-1.643887</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.487086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>-0.007069</td>\n",
       "      <td>0.471844</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>-1.234936</td>\n",
       "      <td>0.473726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8696</th>\n",
       "      <td>0.946269</td>\n",
       "      <td>1.465741</td>\n",
       "      <td>-1.620734</td>\n",
       "      <td>-1.847518</td>\n",
       "      <td>-0.196470</td>\n",
       "      <td>0.654241</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.820403</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.718160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.214481</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>-1.328508</td>\n",
       "      <td>-0.730492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>-0.282872</td>\n",
       "      <td>-0.399890</td>\n",
       "      <td>0.509691</td>\n",
       "      <td>0.479731</td>\n",
       "      <td>-0.397822</td>\n",
       "      <td>-0.267623</td>\n",
       "      <td>0.230450</td>\n",
       "      <td>-0.230343</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.419859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.760314</td>\n",
       "      <td>-0.007444</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>0.379607</td>\n",
       "      <td>0.340119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>1.205448</td>\n",
       "      <td>1.246944</td>\n",
       "      <td>-1.280582</td>\n",
       "      <td>-1.255426</td>\n",
       "      <td>-1.026854</td>\n",
       "      <td>0.424208</td>\n",
       "      <td>1.329968</td>\n",
       "      <td>-0.476772</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>-0.653296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>-0.008337</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>-0.435255</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>-0.004683</td>\n",
       "      <td>-1.501900</td>\n",
       "      <td>-0.469577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>1.066321</td>\n",
       "      <td>0.702654</td>\n",
       "      <td>-0.788257</td>\n",
       "      <td>1.705032</td>\n",
       "      <td>-0.225530</td>\n",
       "      <td>0.313667</td>\n",
       "      <td>-1.340290</td>\n",
       "      <td>2.305823</td>\n",
       "      <td>-0.100504</td>\n",
       "      <td>0.800570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>-0.019280</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.973455</td>\n",
       "      <td>-0.002612</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>1.248138</td>\n",
       "      <td>0.772071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8700 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0.928446  0.666151 -0.131081  1.311142  1.294932  0.125111 -0.913778   \n",
       "1     0.611319  1.955309 -1.094680 -0.588039  0.876183  0.122093 -0.901545   \n",
       "2     1.755482  1.893784  1.159027  0.681295  1.169746  0.114634 -0.643541   \n",
       "3    -1.244110 -1.325378  0.967899  0.858038  1.649315  0.124753 -0.913778   \n",
       "4     0.469802  0.251688  0.792700  0.917488  1.200234  0.117917 -0.141991   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8695 -0.398736 -0.544932 -1.853470 -1.356849 -0.775205  0.248865  0.656794   \n",
       "8696  0.946269  1.465741 -1.620734 -1.847518 -0.196470  0.654241  0.477281   \n",
       "8697 -0.282872 -0.399890  0.509691  0.479731 -0.397822 -0.267623  0.230450   \n",
       "8698  1.205448  1.246944 -1.280582 -1.255426 -1.026854  0.424208  1.329968   \n",
       "8699  1.066321  0.702654 -0.788257  1.705032 -0.225530  0.313667 -1.340290   \n",
       "\n",
       "            V8       V10       V11  ...         0         1         2  \\\n",
       "0     1.299365 -0.147829  1.578256  ... -0.002328 -0.007123 -0.002698   \n",
       "1     0.301063 -0.147829 -0.160771  ...  0.001236 -0.012644 -0.003799   \n",
       "2     0.987774 -0.147829 -0.420515  ... -0.006163 -0.006752  0.006907   \n",
       "3     0.630805 -0.147829  0.643974  ... -0.003119  0.016656  0.012282   \n",
       "4     0.721560 -0.147829 -0.548163  ...  0.000069  0.003833  0.003310   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8695 -1.643887 -0.100504 -0.487086  ...  0.000429  0.002547  0.001180   \n",
       "8696  0.820403 -0.100504 -0.718160  ...  0.002216 -0.010896 -0.000269   \n",
       "8697 -0.230343 -0.100504 -0.419859  ...  0.004427  0.005419 -0.002729   \n",
       "8698 -0.476772 -0.100504 -0.653296  ...  0.001539 -0.008337  0.000296   \n",
       "8699  2.305823 -0.100504  0.800570  ... -0.001394  0.006876  0.004164   \n",
       "\n",
       "             3         4         5         6         7         8         9  \n",
       "0    -0.010898 -0.000392  0.477527 -0.006696  0.014974  2.181365 -1.147782  \n",
       "1    -0.001617  0.002920 -0.566121  0.005820  0.002189 -0.814793 -0.868933  \n",
       "2    -0.011537  0.005116 -1.375672 -0.003718 -0.000003 -0.278375 -1.255553  \n",
       "3    -0.014350  0.003423 -0.312640 -0.016857  0.000189  1.877005  0.491665  \n",
       "4    -0.003955  0.005181 -0.929842 -0.008932 -0.001703  0.458964 -0.206948  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8695  0.007990 -0.007069  0.471844  0.005321 -0.001410 -1.234936  0.473726  \n",
       "8696  0.003975  0.000772 -0.214481  0.011185 -0.001985 -1.328508 -0.730492  \n",
       "8697  0.001570  0.005798 -0.760314 -0.007444 -0.003079  0.379607  0.340119  \n",
       "8698  0.003155  0.001892 -0.435255  0.010791 -0.004683 -1.501900 -0.469577  \n",
       "8699 -0.019280  0.002128  0.973455 -0.002612  0.004673  1.248138  0.772071  \n",
       "\n",
       "[8700 rows x 47 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of SVR: 0.9768388544375323\n",
      "Mean Absolute Error of LogisticAT: 1.143448275862069\n",
      "Mean Absolute Error of RandomForestRegressor: 0.9840728755248335\n",
      "Mean Absolute Error of lassoRegressor: 1.0373314401956168\n",
      "Mean Absolute Error of ElasticNet: 1.1510239288649249\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df,data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mord\n",
    "mae_SVR = []\n",
    "mae_Logitstic_AT = []\n",
    "mae_RandomForestRegressor = []\n",
    "mae_lasso = []\n",
    "mae_ElasticNet = []\n",
    "\n",
    "#SVR\n",
    "SVR_regressor = SVR(kernel = 'rbf').fit(X_train, y_train)\n",
    "mae_SVR.append(mean_absolute_error(SVR_regressor.predict(X_test), y_test))\n",
    "\n",
    "#value must be [0,1]\n",
    "mord_LogisticAT =  mord.LogisticIT(alpha=3).fit(X_train,y_train)\n",
    "mae_Logitstic_AT.append(mean_absolute_error(mord_LogisticAT.predict(X_test), y_test))\n",
    "\n",
    "#RandomForestRegressor\n",
    "RandomForestRegressor = RandomForestRegressor(n_estimators =100, max_depth=10, random_state=0).fit(X_train, y_train)\n",
    "mae_RandomForestRegressor.append(mean_absolute_error(RandomForestRegressor.predict(X_test), y_test))\n",
    "\n",
    "#linear lasso\n",
    "lasso_regressor = linear_model.Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "mae_lasso.append(mean_absolute_error(lasso_regressor.predict(X_test), y_test))\n",
    "\n",
    "#linear-elasticNet\n",
    "ElasticnNet_regressor = linear_model.ElasticNet(random_state = 0).fit(X_train, y_train)\n",
    "mae_ElasticNet.append(mean_absolute_error(ElasticnNet_regressor.predict(X_test), y_test))\n",
    "\n",
    "print('Mean Absolute Error of SVR: %s' % np.mean(mae_SVR))\n",
    "print('Mean Absolute Error of LogisticAT: %s' % np.mean(mae_Logitstic_AT))\n",
    "print('Mean Absolute Error of RandomForestRegressor: %s' % np.mean(mae_RandomForestRegressor))\n",
    "print('Mean Absolute Error of lassoRegressor: %s' % np.mean(mae_lasso))\n",
    "print('Mean Absolute Error of ElasticNet: %s' % np.mean(mae_ElasticNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([pd.DataFrame(Latent2_fea), pd.DataFrame(Latent1_fea)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of SVR: 0.9768388544375323\n",
      "Mean Absolute Error of LogisticAT: 1.1439080459770115\n",
      "Mean Absolute Error of RandomForestRegressor: 0.9840728755248335\n",
      "Mean Absolute Error of lassoRegressor: 1.0373314401956168\n",
      "Mean Absolute Error of ElasticNet: 1.1510239288649249\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df,data.iloc[:, -1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mord\n",
    "mae_SVR = []\n",
    "mae_Logitstic_AT = []\n",
    "mae_RandomForestRegressor = []\n",
    "mae_lasso = []\n",
    "mae_ElasticNet = []\n",
    "\n",
    "#SVR\n",
    "SVR_regressor = SVR(kernel = 'rbf').fit(X_train, y_train)\n",
    "mae_SVR.append(mean_absolute_error(SVR_regressor.predict(X_test), y_test))\n",
    "\n",
    "#value must be [0,1]\n",
    "mord_LogisticAT =  mord.LogisticIT(alpha=1).fit(X_train,y_train)\n",
    "mae_Logitstic_AT.append(mean_absolute_error(mord_LogisticAT.predict(X_test), y_test))\n",
    "\n",
    "#RandomForestRegressor\n",
    "RandomForestRegressor = RandomForestRegressor(n_estimators =100, max_depth=10, random_state=0).fit(X_train, y_train)\n",
    "mae_RandomForestRegressor.append(mean_absolute_error(RandomForestRegressor.predict(X_test), y_test))\n",
    "\n",
    "#linear lasso\n",
    "lasso_regressor = linear_model.Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "mae_lasso.append(mean_absolute_error(lasso_regressor.predict(X_test), y_test))\n",
    "\n",
    "#linear-elasticNet\n",
    "ElasticnNet_regressor = linear_model.ElasticNet(random_state = 0).fit(X_train, y_train)\n",
    "mae_ElasticNet.append(mean_absolute_error(ElasticnNet_regressor.predict(X_test), y_test))\n",
    "\n",
    "print('Mean Absolute Error of SVR: %s' % np.mean(mae_SVR))\n",
    "print('Mean Absolute Error of LogisticAT: %s' % np.mean(mae_Logitstic_AT))\n",
    "print('Mean Absolute Error of RandomForestRegressor: %s' % np.mean(mae_RandomForestRegressor))\n",
    "print('Mean Absolute Error of lassoRegressor: %s' % np.mean(mae_lasso))\n",
    "print('Mean Absolute Error of ElasticNet: %s' % np.mean(mae_ElasticNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8700, 10])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(data_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VAE(input_dim, inter_dim, latent_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-092c58482e8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtrain_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "best_loss = 1e9\n",
    "best_epoch = 0\n",
    "\n",
    "valid_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_num = len(train_loader.dataset)\n",
    "\n",
    "    for idx, (x, _) in enumerate(train_loader):\n",
    "        batch = x.size(0)\n",
    "        x = x.to(device)\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        recon = recon_loss(recon_x, x)\n",
    "        kl = kl_loss(mu, logvar)\n",
    "\n",
    "        loss = recon + kl\n",
    "        train_loss += loss.item()\n",
    "        loss = loss / batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Training loss {loss: .3f} \\t Recon {recon / batch: .3f} \\t KL {kl / batch: .3f} in Step {idx}\")\n",
    "\n",
    "    train_losses.append(train_loss / train_num)\n",
    "\n",
    "    valid_loss = 0.\n",
    "    valid_recon = 0.\n",
    "    valid_kl = 0.\n",
    "    valid_num = len(test_loader.dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, _) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            recon_x, mu, logvar = model(x)\n",
    "            recon = recon_loss(recon_x, x)\n",
    "            kl = kl_loss(mu, logvar)\n",
    "            loss = recon + kl\n",
    "            valid_loss += loss.item()\n",
    "            valid_kl += kl.item()\n",
    "            valid_recon += recon.item()\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        print(f\"Valid loss {valid_loss / valid_num: .3f} \\t Recon {valid_recon / valid_num: .3f} \\t KL {valid_kl / valid_num: .3f} in epoch {epoch}\")\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "            torch.save(model.state_dict(), 'best_model_mnist')\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x217e8876cc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
